{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import copy\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    print('Cuda Available')\n",
    "else:\n",
    "    print('No GPU support currently')\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# upload the database\n",
    "file_name = (r'Database.csv') # insert database name here\n",
    "data = pd.read_csv(file_name)\n",
    "data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop rows with missing values in the target column\n",
    "data = data.dropna(subset=['GEOLOGIAcat'])\n",
    "# Drop rows with Vs higher than 1500 m/s\n",
    "data = data[data['VsEquivalente'] <= 1500]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data filtering function based on Vs values\n",
    "def filtro_vs_intervalli(data):\n",
    "    \n",
    "    data = data[((data['GEOLOGIAcat'] == 1) & (data['VsEquivalente'] >= 200) & (data['VsEquivalente'] <= 375)) |\n",
    "                (data['GEOLOGIAcat'] != 1)]\n",
    "    \n",
    "    \n",
    "    data = data[((data['GEOLOGIAcat'] == 2) & (data['VsEquivalente'] >= 200) & (data['VsEquivalente'] <= 650)) |\n",
    "                (data['GEOLOGIAcat'] != 2)]\n",
    "    \n",
    "    \n",
    "    data = data[((data['GEOLOGIAcat'] == 3) & (data['VsEquivalente'] >= 0) & (data['VsEquivalente'] <= 600)) |\n",
    "                (data['GEOLOGIAcat'] != 3)]\n",
    "    \n",
    "    \n",
    "    data = data[((data['GEOLOGIAcat'] == 4) & (data['VsEquivalente'] >= 350) & (data['VsEquivalente'] <= 600)) |\n",
    "                (data['GEOLOGIAcat'] != 4)]\n",
    "    \n",
    "    \n",
    "    data = data[((data['GEOLOGIAcat'] == 5) & (data['VsEquivalente'] >= 200) & (data['VsEquivalente'] <= 550)) |\n",
    "                (data['GEOLOGIAcat'] != 5)]\n",
    "    \n",
    "    \n",
    "    data = data[((data['GEOLOGIAcat'] == 6) & (data['VsEquivalente'] >= 200) & (data['VsEquivalente'] <= 900)) |\n",
    "                (data['GEOLOGIAcat'] != 6)]\n",
    "    \n",
    "    \n",
    "    data = data[((data['GEOLOGIAcat'] == 7) & (data['VsEquivalente'] >= 0) & (data['VsEquivalente'] <= 500)) |\n",
    "                (data['GEOLOGIAcat'] != 7)]\n",
    "    \n",
    "    \n",
    "    data = data[((data['GEOLOGIAcat'] == 8) & (data['VsEquivalente'] >= 0) & (data['VsEquivalente'] <= 300)) |\n",
    "                (data['GEOLOGIAcat'] != 8)]\n",
    "    \n",
    "    \n",
    "    data = data[((data['GEOLOGIAcat'] == 9) & (data['VsEquivalente'] >= 250) & (data['VsEquivalente'] <= 1000)) |\n",
    "                (data['GEOLOGIAcat'] != 9)]\n",
    "    \n",
    "    \n",
    "    data = data[((data['GEOLOGIAcat'] == 10) & (data['VsEquivalente'] >= 200) & (data['VsEquivalente'] <= 1000)) |\n",
    "                (data['GEOLOGIAcat'] != 10)]\n",
    "    \n",
    "    \n",
    "    data = data[((data['GEOLOGIAcat'] == 11) & (data['VsEquivalente'] >= 50) & (data['VsEquivalente'] <= 800)) |\n",
    "                (data['GEOLOGIAcat'] != 11)]\n",
    "    \n",
    "    \n",
    "    data = data[((data['GEOLOGIAcat'] == 12) & (data['VsEquivalente'] >= 200) & (data['VsEquivalente'] <= 1500)) |\n",
    "                (data['GEOLOGIAcat'] != 12)]\n",
    "    \n",
    "    \n",
    "    data = data[((data['GEOLOGIAcat'] == 13) & (data['VsEquivalente'] >= 500) & (data['VsEquivalente'] <= 1200)) |\n",
    "                (data['GEOLOGIAcat'] != 13)]\n",
    "    \n",
    "    \n",
    "    data = data[((data['GEOLOGIAcat'] == 14) & (data['VsEquivalente'] >= 300) & (data['VsEquivalente'] <= 700)) |\n",
    "                (data['GEOLOGIAcat'] != 14)]\n",
    "    \n",
    "    \n",
    "    data = data[((data['GEOLOGIAcat'] == 15) & (data['VsEquivalente'] >= 200) & (data['VsEquivalente'] <= 700)) |\n",
    "                (data['GEOLOGIAcat'] != 15)]\n",
    "    \n",
    "    \n",
    "    data = data[((data['GEOLOGIAcat'] == 16) & (data['VsEquivalente'] >= 300) & (data['VsEquivalente'] <= 500)) |\n",
    "                (data['GEOLOGIAcat'] != 16)]\n",
    "    \n",
    "    \n",
    "    data = data[((data['GEOLOGIAcat'] == 17) & (data['VsEquivalente'] >= 200) & (data['VsEquivalente'] <= 700)) |\n",
    "                (data['GEOLOGIAcat'] != 17)]\n",
    "    \n",
    "    \n",
    "    data = data[((data['GEOLOGIAcat'] == 18) & (data['VsEquivalente'] >= 200) & (data['VsEquivalente'] <= 900)) |\n",
    "                (data['GEOLOGIAcat'] != 18)]\n",
    "    \n",
    "    \n",
    "    data = data[((data['GEOLOGIAcat'] == 20) & (data['VsEquivalente'] >= 100) & (data['VsEquivalente'] <= 400)) |\n",
    "                (data['GEOLOGIAcat'] != 20)]\n",
    "    \n",
    "    return data\n",
    "\n",
    "# Apply the filtering function to the data\n",
    "data_filtered = filtro_vs_intervalli(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filtering the data based on the 'xcoord' and 'ycoord' columns\n",
    "data_filtered['coord_pair'] = list(zip(data_filtered['xcoord'], data_filtered['xcoord']))\n",
    "\n",
    "# Drop duplicate rows based on the 'coord_pair' column\n",
    "data_final = data_filtered.drop_duplicates(subset='coord_pair').reset_index(drop=True)\n",
    "\n",
    "# Drop the 'coord_pair' column\n",
    "data_final = data_final.drop(columns='coord_pair')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop\n",
    "data_final = data_final.drop(columns=['id', 'spessore_totale', 'tipo_ind'])\n",
    "data_filtered = data_final.dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.utils import resample\n",
    "\n",
    "# Data augmentation function\n",
    "def classify_velocity(value):\n",
    "    if value < 180:\n",
    "        return '<180'\n",
    "    elif 180 <= value < 240:\n",
    "        return '180-240'\n",
    "    elif 240 <= value < 300:\n",
    "        return '240-300'\n",
    "    elif 300 <= value < 360:\n",
    "        return '300-360'\n",
    "    elif 360 <= value < 420:\n",
    "        return '360-420'\n",
    "    elif 420 <= value < 480:\n",
    "        return '420-480'\n",
    "    elif 480 <= value < 560:\n",
    "        return '480-560'\n",
    "    elif 560 <= value < 640:\n",
    "        return '560-640'\n",
    "    elif 640 <= value < 760:\n",
    "        return '640-760'\n",
    "    else:\n",
    "        return '>760'\n",
    "\n",
    "# Add a new column 'Class' to the filtered data\n",
    "data_filtered['Class'] = data_filtered['VsEquivalente'].apply(classify_velocity)\n",
    "\n",
    "# Count the number of samples in each class\n",
    "class_counts = data_filtered['Class'].value_counts()\n",
    "\n",
    "# Define the target class\n",
    "target_class = '360-420'\n",
    "target_class_count = class_counts[target_class]\n",
    "\n",
    "# Find the majority class\n",
    "majority_class = class_counts.idxmax()  \n",
    "majority_class_count = class_counts.max()  \n",
    "\n",
    "# Empty DataFrame to store the balanced data\n",
    "balanced_data = pd.DataFrame()\n",
    "\n",
    "# Under-sampling and over-sampling\n",
    "for class_label, count in class_counts.items():\n",
    "    class_subset = data_filtered[data_filtered['Class'] == class_label]\n",
    "    \n",
    "    if class_label == majority_class:\n",
    "        \n",
    "        class_subset_resampled = resample(class_subset,\n",
    "                                          replace=False,      \n",
    "                                          n_samples=target_class_count, \n",
    "                                          random_state=42)    \n",
    "    elif count < target_class_count:\n",
    "        \n",
    "        class_subset_resampled = resample(class_subset,\n",
    "                                          replace=True,       \n",
    "                                          n_samples=target_class_count, \n",
    "                                          random_state=42)    \n",
    "    else:\n",
    "        \n",
    "        class_subset_resampled = class_subset\n",
    "    \n",
    "    # Add the resampled subset to the balanced data\n",
    "    balanced_data = pd.concat([balanced_data, class_subset_resampled])\n",
    "\n",
    "balanced_data = balanced_data.drop(columns=['Class']).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop rows with missing values\n",
    "balanced_data = balanced_data.dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "\n",
    "# Create a dictionary that maps the lithology code to the lithology name\n",
    "geologia_mapping = {\n",
    "    1: ('Ad', '#7F90E0'),\n",
    "    2: ('CM', '#E87208'),\n",
    "    3: ('Al', '#A4E1FE'),\n",
    "    4: ('Gd', '#160BBF'),\n",
    "    5: ('E', '#F9A906'),\n",
    "    6: ('Ssr', '#4DC79E'),\n",
    "    7: ('Mw', '#000000'),\n",
    "    8: ('Li', '#27CEFB'),\n",
    "    9: ('Lb', '#FA1301'),\n",
    "    10: ('M', '#6D2E07'),\n",
    "    11: ('Pr', '#F47EAB'),\n",
    "    12: ('Cr', '#EBD3BD'),\n",
    "    13: ('Ir', '#CC197D'),\n",
    "    14: ('Nsr', '#EF3DE6'),\n",
    "    15: ('Sr', '#7130FC'),\n",
    "    16: ('Ccr', '#FAE802'),\n",
    "    17: ('Ucr', '#FDFFB7'),\n",
    "    18: ('SM', '#B9E064'),\n",
    "    20: ('B', '#FEFF08')\n",
    "}\n",
    "\n",
    "# Create a dictionary that maps the lithology code to the lithology color\n",
    "geologia_colori = {v[0]: v[1] for v in geologia_mapping.values()}\n",
    "\n",
    "# Create a new column 'GEOLOGIA_sigla' that maps the lithology code to the lithology name\n",
    "balanced_data['GEOLOGIA_sigla'] = balanced_data['GEOLOGIAcat'].map(lambda x: geologia_mapping[x][0])\n",
    "\n",
    "# Calculate the average velocity for each lithology\n",
    "media_velocità = balanced_data.groupby('GEOLOGIA_sigla')['VsEquivalente'].mean().reset_index()\n",
    "\n",
    "# order the lithologies based on the average velocity\n",
    "media_velocità = media_velocità.sort_values(by='VsEquivalente', ascending=False)\n",
    "\n",
    "# Create a list of ordered lithologies\n",
    "ordered_lithologies = media_velocità['GEOLOGIA_sigla'].tolist()\n",
    "\n",
    "# Set the figure \n",
    "plt.figure(figsize=(12, 8))\n",
    "\n",
    "sns.boxplot(x='GEOLOGIA_sigla', y='VsEquivalente', data=balanced_data,\n",
    "            palette=geologia_colori, order=ordered_lithologies)\n",
    "\n",
    "plt.xlabel('Lithological classes', fontsize=14)\n",
    "plt.ylabel('Vs,30 [m/s]', fontsize=14)\n",
    "\n",
    "plt.xticks(rotation=45)\n",
    "plt.grid(alpha=0.1)\n",
    "\n",
    "# Save figure \n",
    "plt.tight_layout()\n",
    "#plt.savefig('box_plot_geologie_after_processing.png', dpi=1000)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to round a number to the nearest multiple of 5\n",
    "def round_to_nearest_5(x):\n",
    "    return round(x / 5) * 5\n",
    "\n",
    "balanced_data['VsEquivalente'] = balanced_data['VsEquivalente'].apply(round_to_nearest_5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Selecting the features and target\n",
    "\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from torch.optim.lr_scheduler import StepLR\n",
    "\n",
    "# Fixing seeds for reproducibility\n",
    "torch.manual_seed(5)\n",
    "torch.cuda.manual_seed(1)\n",
    "torch.cuda.manual_seed_all(1)\n",
    "\n",
    "# convert string to array\n",
    "def convert_string_to_array(string_array):\n",
    "    \n",
    "    if isinstance(string_array, np.ndarray):\n",
    "        return string_array\n",
    "\n",
    "    string_values = string_array.strip('[]').split(', ')\n",
    "    float_array = np.array([float(value) for value in string_values])\n",
    "    return float_array\n",
    "\n",
    "# Starting the scaler\n",
    "scaler = StandardScaler()\n",
    "\n",
    "# features selection\n",
    "features = balanced_data[['DEM1', 'SLOPE1', 'ROUGHNESS1', 'RUGGEDNESS1', 'TOPOGRAPHI1', 'PROFILE_CU1', 'TANGENTIAL_CU1', 'ASPECT1', 'DX1', 'DY1', 'GEOLOGIAcat']]\n",
    "# Target selection\n",
    "labels = balanced_data[['VsEquivalente']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Normalizing the features\n",
    "import pandas as pd\n",
    "import torch\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Upoading the global means and standard deviations for the normalization \n",
    "global_means = pd.read_csv('global_means.csv', index_col=0, usecols=[0, 1])\n",
    "global_stds = pd.read_csv('global_stds.csv', index_col=0, usecols=[0, 1])\n",
    "\n",
    "# Columns to normalize\n",
    "columns_to_normalize = ['DEM1', 'SLOPE1', 'ROUGHNESS1', 'RUGGEDNESS1', \n",
    "                         'TOPOGRAPHI1', 'PROFILE_CU1', 'TANGENTIAL_CU1', \n",
    "                         'ASPECT1', 'DX1', 'DY1']\n",
    "\n",
    "# columns not to normalize\n",
    "columns_non_normalized = features.columns.difference(columns_to_normalize)\n",
    "\n",
    "# Create a StandardScaler object\n",
    "scaler = StandardScaler()\n",
    "scaler.mean_ = global_means.values.flatten()\n",
    "scaler.scale_ = global_stds.values.flatten()\n",
    "\n",
    "features_selected = features[columns_to_normalize]\n",
    "\n",
    "features_normalized = scaler.transform(features_selected.values)\n",
    "\n",
    "# Add the non-normalized columns to the normalized features\n",
    "features_final = pd.DataFrame(features_normalized, columns=columns_to_normalize)\n",
    "features_final[columns_non_normalized] = features[columns_non_normalized].values\n",
    "\n",
    "# 1. Data splitting into training, validation, and test sets\n",
    "features_train, features_temp, labels_train, labels_temp = train_test_split(\n",
    "    features_final.values, labels.values, test_size=0.2, random_state=42)\n",
    "\n",
    "features_val, features_test, labels_val, labels_test = train_test_split(\n",
    "    features_temp, labels_temp, test_size=0.5, random_state=42)\n",
    "\n",
    "# 2. Convert the NumPy arrays to PyTorch tensors\n",
    "features_tensor_train = torch.tensor(features_train, dtype=torch.float32)\n",
    "labels_tensor_train = torch.tensor(labels_train, dtype=torch.float32)\n",
    "\n",
    "features_tensor_val = torch.tensor(features_val, dtype=torch.float32)\n",
    "labels_tensor_val = torch.tensor(labels_val, dtype=torch.float32)\n",
    "\n",
    "features_tensor_test = torch.tensor(features_test, dtype=torch.float32)\n",
    "labels_tensor_test = torch.tensor(labels_test, dtype=torch.float32)\n",
    "\n",
    "# 3. Creation of DataLoaders\n",
    "train_dataset = TensorDataset(features_tensor_train, labels_tensor_train)\n",
    "train_loader = DataLoader(train_dataset, batch_size=1024, shuffle=True)\n",
    "\n",
    "val_dataset = TensorDataset(features_tensor_val, labels_tensor_val)\n",
    "val_loader = DataLoader(val_dataset, batch_size=1024, shuffle=False)\n",
    "\n",
    "test_dataset = TensorDataset(features_tensor_test, labels_tensor_test)\n",
    "test_loader = DataLoader(test_dataset, batch_size=1024, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model definition and Training\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from torch.optim.lr_scheduler import StepLR\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import copy\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score, median_absolute_error, explained_variance_score\n",
    "\n",
    "# Defining the neural network model\n",
    "class NeuralNetwork(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size1, hidden_size2, output_size, dropout_prob=0.01):\n",
    "        super(NeuralNetwork, self).__init__()\n",
    "        self.layer1 = nn.Linear(input_size, hidden_size1)\n",
    "        self.relu1 = nn.ReLU()\n",
    "        self.dropout1 = nn.Dropout(dropout_prob)\n",
    "        self.layer2 = nn.Linear(hidden_size1, hidden_size2)\n",
    "        self.relu2 = nn.ReLU()\n",
    "        self.dropout2 = nn.Dropout(dropout_prob)\n",
    "        self.layer3 = nn.Linear(hidden_size2, output_size)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.layer1(x)\n",
    "        x = self.relu1(x)\n",
    "        x = self.dropout1(x)\n",
    "        x = self.layer2(x)\n",
    "        x = self.relu2(x)\n",
    "        x = self.dropout2(x)\n",
    "        x = self.layer3(x)\n",
    "\n",
    "        smooth_range = (x > 1500)\n",
    "        x = torch.where(smooth_range, 1500 + torch.sigmoid(x - 1500) * (x - 1500), x)\n",
    "        x = torch.minimum(x, torch.tensor(2000.0).to(x.device))  \n",
    "        return x\n",
    "\n",
    "\n",
    "# Model parameters\n",
    "input_size = 11  # Input features\n",
    "output_size = 1  # Output features\n",
    "\n",
    "# Hyperparameters\n",
    "hidden_sizes = [(4096, 8192) ]\n",
    "learning_rates = [0.001]\n",
    "dropout_probs = [0.01]  \n",
    "l1_lambdas = [0.00001]  \n",
    "l2_lambdas = [0.00001]  \n",
    "best_model = None\n",
    "best_loss = float('inf')\n",
    "best_configs = []\n",
    "\n",
    "# Regularized loss function\n",
    "def loss_with_regularization(outputs, labels, l1_lambda, l2_lambda):\n",
    "    l1_reg = 0\n",
    "    l2_reg = 0\n",
    "    for param in model.parameters():\n",
    "        l1_reg += torch.norm(param, 1)\n",
    "        l2_reg += torch.norm(param, 2)\n",
    "    loss = criterion(outputs, labels)\n",
    "    loss += l1_lambda * l1_reg\n",
    "    loss += l2_lambda * l2_reg\n",
    "    return loss\n",
    "\n",
    "# Early stopping parameters\n",
    "patience = 50\n",
    "patience_counter = 0\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "\n",
    "for (hidden_size1, hidden_size2) in hidden_sizes:\n",
    "    for lr in learning_rates:\n",
    "        for dropout_prob in dropout_probs:\n",
    "            for l1_lambda in l1_lambdas:\n",
    "                for l2_lambda in l2_lambdas:\n",
    "                    \n",
    "                    model = NeuralNetwork(input_size, hidden_size1, hidden_size2, output_size, dropout_prob).to(device)\n",
    "\n",
    "                    \n",
    "                    criterion = nn.MSELoss()\n",
    "                    optimizer = optim.Adam(model.parameters(), lr=lr, weight_decay=l2_lambda)\n",
    "                    scheduler = StepLR(optimizer, step_size=100, gamma=0.5)\n",
    "\n",
    "                    train_losses_config = []\n",
    "                    val_losses_config = []\n",
    "\n",
    "                    # Training loop\n",
    "                    num_epochs = 5000\n",
    "                    for epoch in range(num_epochs):\n",
    "                        model.train()\n",
    "                        train_loss_epoch = 0.0\n",
    "                        for inputs, labels in train_loader:\n",
    "                            optimizer.zero_grad()\n",
    "                            inputs, labels = inputs.to(device), labels.to(device)\n",
    "                            outputs = model(inputs)\n",
    "                            loss = loss_with_regularization(outputs, labels, l1_lambda, l2_lambda)\n",
    "                            loss.backward()\n",
    "                            optimizer.step()\n",
    "                            train_loss_epoch += loss.item()\n",
    "                        scheduler.step()\n",
    "                        model.eval()\n",
    "\n",
    "                        train_loss_epoch /= len(train_loader)\n",
    "                        train_losses_config.append(train_loss_epoch)\n",
    "\n",
    "                        val_loss_epoch = 0.0\n",
    "                        num_batches_val = 0\n",
    "                        all_outputs = []\n",
    "                        all_labels = []\n",
    "                        with torch.no_grad():\n",
    "                            for inputs, labels in val_loader:\n",
    "                                inputs, labels = inputs.to(device), labels.to(device)\n",
    "                                outputs = model(inputs)\n",
    "                                val_loss_epoch += criterion(outputs, labels).item()\n",
    "                                num_batches_val += 1\n",
    "                                all_outputs.append(outputs.cpu().numpy())\n",
    "                                all_labels.append(labels.cpu().numpy())\n",
    "\n",
    "                        val_loss_epoch /= num_batches_val\n",
    "                        val_losses_config.append(val_loss_epoch)\n",
    "\n",
    "                        # Concatenate all outputs and labels for metrics calculation\n",
    "                        all_outputs = np.concatenate(all_outputs)\n",
    "                        all_labels = np.concatenate(all_labels)\n",
    "\n",
    "                        # Print the loss every 10 epochs\n",
    "                        if (epoch + 1) % 10 == 0:\n",
    "                            print(f'Epoch [{epoch + 1}/{num_epochs}], Loss: {train_loss_epoch:.4f}, Val Loss: {val_loss_epoch:.4f}, '\n",
    "                                  f'Config: H1={hidden_size1}, H2={hidden_size2}, LR={lr}, Dropout={dropout_prob}, '\n",
    "                                  f'L1={l1_lambda}, L2={l2_lambda}')\n",
    "\n",
    "                        # Early stopping check\n",
    "                        if val_loss_epoch < best_loss:\n",
    "                            best_loss = val_loss_epoch\n",
    "                            best_epoch = epoch\n",
    "                            best_model = copy.deepcopy(model)  # Save the best model\n",
    "                            patience_counter = 0\n",
    "                        else:\n",
    "                            patience_counter += 1\n",
    "                            if patience_counter >= patience:\n",
    "                                print(f\"Early stopping at epoch {epoch + 1}\")\n",
    "                                break\n",
    "\n",
    "                    # Use the best model for evaluation\n",
    "                    model = best_model  \n",
    "\n",
    "                    # Statistics calculation\n",
    "                    mae = mean_absolute_error(all_labels, all_outputs)\n",
    "                    mse = mean_squared_error(all_labels, all_outputs)\n",
    "                    rmse = np.sqrt(mse)\n",
    "                    r2 = r2_score(all_labels, all_outputs)\n",
    "                    median_ae = median_absolute_error(all_labels, all_outputs)\n",
    "                    explained_variance = explained_variance_score(all_labels, all_outputs)\n",
    "                    max_error = np.max(np.abs(all_labels - all_outputs))\n",
    "\n",
    "                    print(f'MAE: {mae:.4f}, MSE: {mse:.4f}, RMSE: {rmse:.4f}, R²: {r2:.4f}, Median AE: {median_ae:.4f}, Explained Variance: {explained_variance:.4f}, Max Error: {max_error:.4f}')\n",
    "\n",
    "                    best_configs.append({\n",
    "                        'hidden_size1': hidden_size1,\n",
    "                        'hidden_size2': hidden_size2,\n",
    "                        'learning_rate': lr,\n",
    "                        'dropout_prob': dropout_prob,\n",
    "                        'l1_lambda': l1_lambda,\n",
    "                        'l2_lambda': l2_lambda,\n",
    "                        'val_loss': val_loss_epoch,\n",
    "                        'mae': mae,\n",
    "                        'rmse': rmse,\n",
    "                        'r2': r2\n",
    "                    })\n",
    "\n",
    "                    # plot the loss\n",
    "                    plt.plot(range(1, len(train_losses_config) + 1), train_losses_config,\n",
    "                             label=f'Train Loss (H1={hidden_size1}, H2={hidden_size2}, LR={lr})', color='blue')\n",
    "                    plt.plot(range(1, len(val_losses_config) + 1), val_losses_config,\n",
    "                             linestyle='--', label=f'Validation Loss (H1={hidden_size1}, H2={hidden_size2}, LR={lr})', color='red')\n",
    "\n",
    "# add minumum validation loss point \n",
    "min_val_loss_epoch = np.argmin(val_losses_config) + 1\n",
    "min_val_loss = min(val_losses_config)\n",
    "plt.scatter(min_val_loss_epoch, min_val_loss, color='red', marker='o', s=100,\n",
    "            label=f'Min Val Loss (Epoch {min_val_loss_epoch})')\n",
    "\n",
    "# Visualize the plot\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss')\n",
    "plt.title('Training and Validation Loss Over Epochs')\n",
    "\n",
    "plt.legend(loc='upper right')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.grid()\n",
    "\n",
    "# Save the plot\n",
    "#plt.savefig('loss_plot.png', dpi=300)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate the model\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "\n",
    "def evaluate_model(model, dataloader, criterion, device):\n",
    "    model.eval()\n",
    "    total_loss = 0.0\n",
    "    predictions = []\n",
    "    true_labels = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for inputs, labels in dataloader:\n",
    "            inputs, labels = inputs.to(device), labels.to(device)\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, labels)\n",
    "            total_loss += loss.item()\n",
    "\n",
    "            predictions.extend(outputs.cpu().numpy())\n",
    "            true_labels.extend(labels.cpu().numpy())\n",
    "\n",
    "    average_loss = total_loss / len(dataloader)\n",
    "    rmse = np.sqrt(mean_squared_error(true_labels, predictions))\n",
    "    r2 = r2_score(true_labels, predictions)\n",
    "\n",
    "    return average_loss, rmse, r2, predictions, true_labels\n",
    "\n",
    "# Evaluate the best model on the training, validation, and test sets\n",
    "train_loss, train_rmse, train_r2, train_predictions, train_true_labels = evaluate_model(best_model, train_loader, nn.MSELoss(), device)\n",
    "val_loss, val_rmse, val_r2, val_predictions, val_true_labels = evaluate_model(best_model, val_loader, nn.MSELoss(), device)\n",
    "test_loss, test_rmse, test_r2, test_predictions, test_true_labels = evaluate_model(best_model, test_loader, nn.MSELoss(), device)\n",
    "\n",
    "print(f'Train Loss: {train_loss:.4f}, Train RMSE: {train_rmse:.4f}, Train R^2 Score: {train_r2:.4f}')\n",
    "print(f'Validation Loss: {val_loss:.4f}, Validation RMSE: {val_rmse:.4f}, Validation R^2 Score: {val_r2:.4f}')\n",
    "print(f'Test Loss: {test_loss:.4f}, Test RMSE: {test_rmse:.4f}, Test R^2 Score: {test_r2:.4f}')\n",
    "\n",
    "# Function to plot the real vs. predicted values\n",
    "def plot_real_vs_predicted(true_labels, predictions, title):\n",
    "    plt.figure(figsize=(8, 8))\n",
    "    plt.scatter(true_labels, predictions, color='blue', alpha=0.7, edgecolor='black')\n",
    "    plt.plot([min(true_labels), max(true_labels)], [min(true_labels), max(true_labels)], linestyle='--', color='red', linewidth=2)\n",
    "    plt.xlabel('True Values')\n",
    "    plt.ylabel('Predicted Values')\n",
    "    plt.title(title)\n",
    "    plt.grid(True)\n",
    "    plt.show()\n",
    "\n",
    "# training set\n",
    "plot_real_vs_predicted(train_true_labels, train_predictions, 'Real vs. Predicted (Training Set)')\n",
    "\n",
    "# validation set\n",
    "plot_real_vs_predicted(val_true_labels, val_predictions, 'Real vs. Predicted (Validation Set)')\n",
    "\n",
    "# test set\n",
    "plot_real_vs_predicted(test_true_labels, test_predictions, 'Real vs. Predicted (Test Set)')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Salva il modello addestrato in UTM\n",
    "import json\n",
    "\n",
    "torch.save(best_model.state_dict(), 'modello_final_100m_30_10_24_HIDDEN2_base_8mila_neuroni_DEM_SLOPE.pth')\n",
    "\n",
    "# 2. Salva gli iperparametri e altre informazioni utili\n",
    "model_info = {\n",
    "    'hidden_size1': hidden_size1,\n",
    "    'hidden_size2': hidden_size2,\n",
    "    'learning_rate': lr,\n",
    "    'best_loss': best_loss,\n",
    "    'best_epoch': best_epoch,\n",
    "}\n",
    "with open('modello_final_100m_30_10_24_HIDDEN2_base_8mila_neuroni_DEM_SLOPE', 'w') as json_file:\n",
    "    json.dump(model_info, json_file)\n",
    "\n",
    "# 3. Salva lo stato dell'ottimizzatore\n",
    "torch.save(optimizer.state_dict(), 'optimizer_state_modello_final_100m_30_10_24_HIDDEN2_base_8mila_neuroni_DEM_SLOPE.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the model, hyperparameters, and optimizer state\n",
    "\n",
    "import json\n",
    "import torch\n",
    "\n",
    "# 1. Save the trained model\n",
    "try:\n",
    "    torch.save(best_model.state_dict(), 'INSERT_NAME_HERE.pth') # insert pth file name here\n",
    "    print(\"Model saved.\")\n",
    "except Exception as e:\n",
    "    print(f\"Error during saving: {e}\")\n",
    "\n",
    "# 2. Save the hyperparameters and other useful information\n",
    "model_info = {\n",
    "    'hidden_size1': hidden_size1,\n",
    "    'hidden_size2': hidden_size2,\n",
    "    'learning_rate': lr,\n",
    "    'best_loss': best_loss,\n",
    "    'best_epoch': best_epoch,\n",
    "}\n",
    "\n",
    "try:\n",
    "    with open('INSERT_NAME_HERE.json', 'w') as json_file: # insert json file name here\n",
    "        json.dump(model_info, json_file)\n",
    "    print(\"Model info saved.\")\n",
    "except Exception as e:\n",
    "    print(f\"Error during saving: {e}\")\n",
    "\n",
    "# 3. Save the optimizer state\n",
    "try:\n",
    "    torch.save(optimizer.state_dict(), 'INSERT_NAME_HERE.pth') # optimizer\n",
    "    print(\"Optimizer saved.\")\n",
    "except Exception as e:\n",
    "    print(f\"Error during saving the optimizer: {e}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

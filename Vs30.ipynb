{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "824ab37c-65ba-40d3-bb13-457aa0068810",
   "metadata": {},
   "outputs": [],
   "source": [
    "### non cancellare...importa su tutti i file\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    print('Cuda Available')\n",
    "else:\n",
    "    print('No GPU support currently')\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0090340b-ffac-4d66-902c-15086e3b486d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "import copy\n",
    "\n",
    "# Caricamento e filtraggio dei dati\n",
    "file_name = ('DB_FINALE_CORRETTO_PER_DEEPL_con_proxy_morfologici_RSLOPEaspect.csv')\n",
    "data = pd.read_csv(file_name)\n",
    "\n",
    "# Rimuovi le righe con valori NaN\n",
    "data_cleaned = data.dropna()\n",
    "data_filtered = data_cleaned[data_cleaned['spessore'] > 29]\n",
    "\n",
    "# Creazione dei nuovi database con mutazioni\n",
    "data_base = data_filtered.copy()\n",
    "data_base['Database'] = 'Base'\n",
    "\n",
    "data_quota_mutata = data_filtered.copy()\n",
    "data_quota_mutata['DEM'] = 0\n",
    "data_quota_mutata['Database'] = 'QuotaMutata'\n",
    "\n",
    "data_slope_mutato = data_filtered.copy()\n",
    "data_slope_mutato['Slope'] = 0\n",
    "data_slope_mutato['Database'] = 'SlopeMutato'\n",
    "\n",
    "data_roughness_mutato = data_filtered.copy()\n",
    "data_roughness_mutato['Roughness'] = 0\n",
    "data_roughness_mutato['Database'] = 'RoughnessMutato'\n",
    "\n",
    "data_ruggedness_mutato = data_filtered.copy()\n",
    "data_ruggedness_mutato['Ruggedness'] = 0\n",
    "data_ruggedness_mutato['Database'] = 'RuggednessMutato'\n",
    "\n",
    "data_Topographic_position_index1_mutato = data_filtered.copy()\n",
    "data_Topographic_position_index1_mutato['Topographic_position_index'] = 0\n",
    "data_Topographic_position_index1_mutato['Database'] = 'Topographic_position_index1_Mutato'\n",
    "\n",
    "data_Profile_curvature_mutato = data_filtered.copy()\n",
    "data_Profile_curvature_mutato['Profile_curvature'] = 0\n",
    "data_Profile_curvature_mutato['Database'] = 'Profile_curvature'\n",
    "\n",
    "data_Tangential_curvature_mutato = data_filtered.copy()\n",
    "data_Tangential_curvature_mutato['Tangential_curvature'] = 0\n",
    "data_Tangential_curvature_mutato['Database'] = 'Tangential_curvature'\n",
    "\n",
    "data_Aspect_mutato = data_filtered.copy()\n",
    "data_Aspect_mutato['Aspect'] = 0\n",
    "data_Aspect_mutato['Database'] = 'Aspect'\n",
    "\n",
    "data_Dx_mutato = data_filtered.copy()\n",
    "data_Dx_mutato['Dx'] = 0\n",
    "data_Dx_mutato['Database'] = 'Dx'\n",
    "\n",
    "data_Dy_mutato = data_filtered.copy()\n",
    "data_Dy_mutato['Dy'] = 0\n",
    "data_Dy_mutato['Database'] = 'Dy'\n",
    "\n",
    "# Concatenazione dei database, NB ricorda che questo db è x5 non x3\n",
    "data_triplicato = pd.concat([data_base, data_quota_mutata, data_slope_mutato,data_roughness_mutato,data_ruggedness_mutato,data_Topographic_position_index1_mutato, \n",
    "                             data_Profile_curvature_mutato, data_Tangential_curvature_mutato, data_Aspect_mutato, data_Dx_mutato, data_Dy_mutato], ignore_index=True)bb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f7a1152-eed2-4a2b-b20c-1083b478fccb",
   "metadata": {},
   "outputs": [],
   "source": [
    "#### versione 1.2 con 2 strati hidden\n",
    "\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from torch.optim.lr_scheduler import StepLR\n",
    "\n",
    "# Estrai le colonne necessarie (latitudine, longitudine, quota, tipo di roccia, slope, Vs30)\n",
    "features = data_triplicato[['Acoord_X', 'Acoord_Y', 'DEM', 'Geologia', 'Slope', 'Roughness', 'Ruggedness', 'Topographic_position_index',\n",
    "                           'Profile_curvature', 'Tangential_curvature', 'Aspect', 'Dx', 'Dy']]\n",
    "labels = data_triplicato[['Vs']]\n",
    "\n",
    "# Normalizzazione dei dati\n",
    "scaler = StandardScaler()\n",
    "features_normalized = scaler.fit_transform(features.values) \n",
    "\n",
    "# Divisione dei dati in set di addestramento, validazione e test\n",
    "features_train, features_temp, labels_train, labels_temp = train_test_split(\n",
    "    features_normalized, labels.values, test_size=0.2, random_state=42)\n",
    "\n",
    "features_val, features_test, labels_val, labels_test = train_test_split(\n",
    "    features_temp, labels_temp, test_size=0.5, random_state=42)\n",
    "\n",
    "# Converti i dati in tensori PyTorch\n",
    "features_tensor_train = torch.tensor(features_train, dtype=torch.float32)\n",
    "labels_tensor_train = torch.tensor(labels_train, dtype=torch.float32)\n",
    "features_tensor_val = torch.tensor(features_val, dtype=torch.float32)\n",
    "labels_tensor_val = torch.tensor(labels_val, dtype=torch.float32)\n",
    "features_tensor_test = torch.tensor(features_test, dtype=torch.float32)\n",
    "labels_tensor_test = torch.tensor(labels_test, dtype=torch.float32)\n",
    "\n",
    "# Creazione del DataLoader per gestire i dati di addestramento\n",
    "train_dataset = TensorDataset(features_tensor_train, labels_tensor_train)\n",
    "train_loader = DataLoader(train_dataset, batch_size=64, shuffle=True)\n",
    "\n",
    "# Creazione di un DataLoader per i dati di validazione\n",
    "val_dataset = TensorDataset(features_tensor_val, labels_tensor_val)\n",
    "val_loader = DataLoader(val_dataset, batch_size=64, shuffle=False)\n",
    "\n",
    "# Creazione di un DataLoader per i dati di test\n",
    "test_dataset = TensorDataset(features_tensor_test, labels_tensor_test)\n",
    "test_loader = DataLoader(test_dataset, batch_size=64, shuffle=False)\n",
    "\n",
    "# Definisci la rete neurale con dropout\n",
    "class NeuralNetwork(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size1, hidden_size2, output_size, dropout_prob=0.01):\n",
    "        super(NeuralNetwork, self).__init__()\n",
    "        self.layer1 = nn.Linear(input_size, hidden_size1)\n",
    "        self.relu1 = nn.ReLU()\n",
    "        self.dropout1 = nn.Dropout(dropout_prob)\n",
    "        self.layer2 = nn.Linear(hidden_size1, hidden_size2)\n",
    "        self.relu2 = nn.ReLU()\n",
    "        self.dropout2 = nn.Dropout(dropout_prob)\n",
    "        self.layer3 = nn.Linear(hidden_size2, output_size)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.layer1(x)\n",
    "        x = self.relu1(x)\n",
    "        x = self.dropout1(x)\n",
    "        x = self.layer2(x)\n",
    "        x = self.relu2(x)\n",
    "        x = self.dropout2(x)\n",
    "        x = self.layer3(x)\n",
    "        return x\n",
    "\n",
    "# Parametri della rete\n",
    "l1_lambda = 0.00001\n",
    "l2_lambda = 0.00001\n",
    "input_size = 13  # Numero di features di input\n",
    "output_size = 1  # Output è la stima della Vs30\n",
    "\n",
    "# Tuning degli iperparametri\n",
    "hidden_sizes = [(4096, 8192)]\n",
    "learning_rates = [0.01]\n",
    "best_model = None\n",
    "best_loss = float('inf')\n",
    "\n",
    "# Colori diversi per le curve\n",
    "colors = ['red', 'blue', 'green', 'purple', 'orange']\n",
    "\n",
    "diagnostic = []\n",
    "\n",
    "# Aggiungi la regolarizzazione L1 e L2 al calcolo della perdita\n",
    "def loss_with_regularization(outputs, labels, l1_lambda, l2_lambda):\n",
    "    l1_reg = 0\n",
    "    l2_reg = 0\n",
    "    for param in model.parameters():\n",
    "        l1_reg += torch.norm(param, 1)\n",
    "        l2_reg += torch.norm(param, 2)\n",
    "    loss = criterion(outputs, labels)\n",
    "    loss += l1_lambda * l1_reg\n",
    "    loss += l2_lambda * l2_reg\n",
    "    return loss\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "for i, (hidden_size1, hidden_size2, ) in enumerate(hidden_sizes):\n",
    "  print(hidden_size1,hidden_size2,)\n",
    "  for j, lr in enumerate(learning_rates):\n",
    "        color_index = i * len(learning_rates) + j\n",
    "        if color_index < len(colors):\n",
    "            color = colors[color_index]\n",
    "        else:\n",
    "            color = 'black'  # Usa il nero se non ci sono abbastanza colori definiti\n",
    "        ### da quì metti tutto in a function\n",
    "        # Creazione della rete con i parametri corretti\n",
    "        model = NeuralNetwork(input_size, hidden_size1, hidden_size2, output_size).to(device)\n",
    "\n",
    "        # Definizione della funzione di perdita e dell'ottimizzatore\n",
    "        criterion = nn.MSELoss()\n",
    "        optimizer = optim.Adam(model.parameters(), lr=lr, weight_decay=l2_lambda)\n",
    "\n",
    "        # Definisci il learning rate scheduler\n",
    "        scheduler = StepLR(optimizer, step_size=100, gamma=0.1)  # Diminuisci il tasso di apprendimento ogni 100 epoche\n",
    "\n",
    "        # Liste separate per la configurazione corrente\n",
    "        train_losses_config = []\n",
    "        val_losses_config = []\n",
    "\n",
    "        # Addestramento della rete con regolarizzazione e scheduler\n",
    "        num_epochs = 500 # 50 era la prima....poi 200\n",
    "        for epoch in range(num_epochs):\n",
    "            model.train()  \n",
    "            train_loss_epoch = 0.0  \n",
    "            for inputs, labels in train_loader:\n",
    "                optimizer.zero_grad()\n",
    "                inputs, labels = inputs.to(device), labels.to(device)  # Move tensors to the configured device\n",
    "                outputs = model(inputs)\n",
    "                loss = loss_with_regularization(outputs, labels, l1_lambda, l2_lambda)  # Pass outputs and labels to the function\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "                train_loss_epoch += loss.item()\n",
    "            scheduler.step()  # Applica il learning rate scheduler\n",
    "            model.eval()\n",
    "\n",
    "            # Calcola la perdita media sul set di addestramento per questa epoca\n",
    "            train_loss_epoch /= len(train_loader)\n",
    "            train_losses_config.append(train_loss_epoch)\n",
    "\n",
    "            # Calcola la perdita sul set di validazione\n",
    "            val_loss_epoch = 0.0  # Resetta la perdita ad ogni epoca\n",
    "            num_batches_val = 0\n",
    "            with torch.no_grad():\n",
    "                for inputs, labels in val_loader:\n",
    "                    inputs,labels = inputs.to(device),labels.to(device)\n",
    "                    outputs = model(inputs)\n",
    "                    val_loss_epoch += criterion(outputs, labels).item()\n",
    "                    num_batches_val += 1\n",
    "\n",
    "            # Calcola la perdita media sul set di validazione per questa epoca\n",
    "            val_loss_epoch /= num_batches_val\n",
    "            val_losses_config.append(val_loss_epoch)\n",
    "\n",
    "            if (epoch + 1) % 10 == 0:\n",
    "                print(f'Epoch [{epoch+1}/{num_epochs}], Loss: {train_loss_epoch:.4f}')\n",
    "\n",
    "            # Valuta le prestazioni del modello sul set di validazione\n",
    "            if val_loss_epoch < best_loss:\n",
    "                best_loss = val_loss_epoch\n",
    "                best_epoch = epoch\n",
    "                best_model = copy.deepcopy(model)\n",
    "            # return best loss, best epoch, model? \n",
    "        diagnostic.append([i, lr, best_loss, best_epoch])\n",
    "        # Visualizza la perdita su train e validation per ogni configurazione\n",
    "        plt.plot(range(1, num_epochs + 1), train_losses_config, label=f'Train Loss (H1={hidden_size1}, H2={hidden_size2}, LR={lr})', color=color)\n",
    "        plt.plot(range(1, num_epochs + 1), val_losses_config, linestyle='--', label=f'Validation Loss (H1={hidden_size1}, H2={hidden_size2}, LR={lr})', color=color)\n",
    "\n",
    "        # Aggiungi un marcatore per il punto di minima perdita sul set di validazione\n",
    "        min_val_loss_epoch = np.argmin(val_losses_config) + 1\n",
    "        min_val_loss = min(val_losses_config)\n",
    "        plt.scatter(min_val_loss_epoch, min_val_loss, color=color, marker='o', s=100,\n",
    "                    label=f'Min Val Loss (H1={hidden_size1}, H2={hidden_size2}, LR={lr})')\n",
    "\n",
    "# Visualizza i grafici di perdita\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss')\n",
    "\n",
    "# Posiziona la legenda fuori dal grafico\n",
    "plt.legend(bbox_to_anchor=(1.05, 1), loc='upper left')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c255afad-95a5-4af5-930b-29694045bb33",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "\n",
    "def evaluate_model(model, dataloader, criterion, device):\n",
    "    model.eval()\n",
    "    total_loss = 0.0\n",
    "    predictions = []\n",
    "    true_labels = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for inputs, labels in dataloader:\n",
    "            inputs, labels = inputs.to(device), labels.to(device)\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, labels)\n",
    "            total_loss += loss.item()\n",
    "\n",
    "            predictions.extend(outputs.cpu().numpy())\n",
    "            true_labels.extend(labels.cpu().numpy())\n",
    "\n",
    "    average_loss = total_loss / len(dataloader)\n",
    "    rmse = np.sqrt(mean_squared_error(true_labels, predictions))\n",
    "    r2 = r2_score(true_labels, predictions)\n",
    "\n",
    "    return average_loss, rmse, r2, predictions, true_labels\n",
    "\n",
    "# Valuta il modello sui set di addestramento, validazione e test\n",
    "train_loss, train_rmse, train_r2, train_predictions, train_true_labels = evaluate_model(best_model, train_loader, nn.MSELoss(), device)\n",
    "val_loss, val_rmse, val_r2, val_predictions, val_true_labels = evaluate_model(best_model, val_loader, nn.MSELoss(), device)\n",
    "test_loss, test_rmse, test_r2, test_predictions, test_true_labels = evaluate_model(best_model, test_loader, nn.MSELoss(), device)\n",
    "\n",
    "print(f'Train Loss: {train_loss:.4f}, Train RMSE: {train_rmse:.4f}, Train R^2 Score: {train_r2:.4f}')\n",
    "print(f'Validation Loss: {val_loss:.4f}, Validation RMSE: {val_rmse:.4f}, Validation R^2 Score: {val_r2:.4f}')\n",
    "print(f'Test Loss: {test_loss:.4f}, Test RMSE: {test_rmse:.4f}, Test R^2 Score: {test_r2:.4f}')\n",
    "\n",
    "# Plotta i valori previsti rispetto ai valori reali per il set di test\n",
    "plt.figure(figsize=(8, 8))\n",
    "plt.scatter(test_true_labels, test_predictions, color='blue', alpha=0.7, edgecolor='black')\n",
    "plt.plot([min(test_true_labels), max(test_true_labels)], [min(test_true_labels), max(test_true_labels)], linestyle='--', color='red', linewidth=2)\n",
    "plt.xlabel('True value')\n",
    "plt.ylabel('Predicted value')\n",
    "plt.title('Scatter plot Real vs. Predicted (Test Set)')\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
